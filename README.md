# CopyCat

Simple scraping tool that I threw together to clone a website given a list of URLs.

---

## OS & Python Version Info

Distributor ID: *Kali*
Description:    *Kali GNU/Linux Rolling*
Release:        *2022.2*
Codename:       *kali-rolling*

*Tested using Python 3.10.4*

---

**Installation**

```bash

python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

```

---

## Generating URLs Input Data

The URLs file that is expected to be passed in should be a list of URLs delimited by a newline character.

How you generate that list is up to you.

I used **BurpSuite** as a proxy and turned off the intercept.  Then I just browsed the site I chose manually, clicking as many links as I could find.  Then in the *Target* tab in BurpSuite, I find the site I was browsing and *Add to scope*.  Then in the *Filter Settings*, I clicked on *Show All* and also then checked the box labeled *Show only in-scope items* and clicked apply.  Then in the pane on the left side, I right click on the site I browsed and choose the option *Copy URLs in this host* and pasted those URLs into a file.  That file can then be used to clone the site.  Then I just moved the entire directory into my Apache server's document root folder and started the service.  You may need to account for some of the redirects which may be custom to the site as well, which can be easily done by analyzing the resulting output file generated by the **copycat** script (just look for anything with a 301 or 302 status code).

---

## Example Usage

```bash

python copycat urls.txt -o copycat.out -d www

```

**Sample Output**

```bash

ls
# copycat.out  copycat.py  README.md  requirements.txt  urls.txt  venv  www

cat copycat.out
# [+] (200) www/index.html
# [*] (301) /assets -> /assets/
# [*] (302) /uploads/ -> /home
# [!] (N/A) https://www.example.com/manifest.json
# ...

tree www
# www
# |--- assets
# |    |--- main.js
# |    |--- main.css
# |--- index.html
# |--- uploads
# |    |--- image1.png
# |    |--- image2.png
#
# 2 directories, 5 files

```

The `urls.txt` parameter is the file containing a list of URLs delimited by a newline, this is a required argument.

The `-o` flag by default will use the output file name of **copycat.out** and is not a required flag.

The `-d` flag by default will use the document root directory name **www** and is not a required flag.

After executing the script using the example above, the results of the scrape can be viewed in the *copycat.out* file and the cloned website files can be found in the *www* directory.

---

**This application was designed for educational purposes ONLY. I am not responsible for any misuse of the application or legal issues that arise due to you're own decisions.**
